%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for creating scribe notes for cos511.
%
%  Fill in your name, lecture number, lecture date and body
%  of scribe notes as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[12pt]{article}

\usepackage{amsmath, amsfonts, amsthm, fullpage, amssymb, algpseudocode, algorithm, graphicx}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}
\setlength{\headsep}{0pt}
\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

\newcommand{\comment}[1]{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{invariant}[theorem]{Invariant}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{property}[theorem]{Property}
\newtheorem{remark}[theorem]{Remark}

\pagestyle{plain}

\begin{document}

\thispagestyle{empty}

\begin{center}
\bf\large COS 511: Theoretical Machine Learning
\end{center}

\noindent
Lecturer: Rob Schapire   %%% FILL IN LECTURER (if not RS)
\hfill
Project Report              %%% FILL IN LECTURE NUMBER HERE
\\
Authors: Christian Eubank, Sachin Ravi                %%% FILL IN YOUR NAME HERE
\hfill
May 14, 2013           %%% FILL IN LECTURE DATE HERE

\noindent
\rule{\textwidth}{1pt}

\medskip

\section{Introduction}
In this paper, we study the stochastic multi-armed bandit problem. Here, a player is  
faced with a row of slot machines (``arms'') and must decide in each round which machine
to play.  Each of the arms when picked provides a random reward to the player from the distribution
specific to the machine. The goal of the player is to maximize the sum of rewards received through a
sequence of selections.  

These problems are the most basic examples of decision games where the player is faced with an 
exploration-exploitation faceoff.  Exploitation is routinely picking the arm that gives a high reward,
whereas exploration involves trying out new arms which may or may not give a higher reward. 

Though the original motivation (which still remains relevant) for the problem was clinical trials, many 
new applications have arisen.  One application is in ad placement, where we want to decide which advertisement
to show the next visitor to a website.  For example, in \cite{Chakrabarti}, the authors formulate a modified version
of the problem, where arms have a lifetime after which they expire.  This expiration helps model ads, which often 
have limited lifetimes due to their content and campaign budgets.  Another application is in Web document ranking.  
Here, the user searches for a topic and relevant documents must be ranked to be shown to the user.  In \cite{Radlinski}, 
the authors propose an online learning method using multi-armed bandit algorithms that produces rankings from usage data.   

\section{Background}
We now review the fundamental formalization of the bandit problem. The problem consists of a set of $K$ probability distributions
$(D_1, \ldots, D_k)$ with associated expected values $(\mu_1, \ldots, \mu_k)$ and variances $(\sigma^2_1, \ldots, \sigma^2_k)$.
In the beginning, the $D_i$ are unknown to the player.  At each round $t = 1,2,\ldots,T$, the player selects an arm with index $j(t)$
and receives a reward $r(t) \sim D_{j(t)}$ and player's goal is to maximize his received reward summed over all rounds.  A bandit algorithm
specifies a strategy by which the player should choose an arm $j(t)$ in each round.

The most popular performance measure (according to \cite{Kuleshov}) for bandit algorithms is \emph{total expected regret}.  Assuming the 
total number of rounds is $T$, \emph{total expected regret} is defined as
\begin{align}
R_T = T\mu^{*} - \sum_{i=1}^{T} \mu_{j(t)},
\end{align}
wher $u^{*} = \max(\mu_1, \ldots, \mu_k)$ is the highest expected reward from the best arm. 

We now present three previously conceived algorithms whose performance we use for comparison in our experiments:  $\epsilon$-greedy, Boltzmann exploration, and Poker.
In the following, let $p_i(t)$ be the probability of picking arm $i$ at time $t$.

\subsection{$\epsilon$-greedy}
The $\epsilon$-greedy algorithm is widely used because it is a simple heuristic that performs great in practice. 
At each round $t = 1, \ldots, T$, the algorithm selects the arm with the highest observed empirical mean with probability $1 - \epsilon$ and
selects a random arm with probability $\epsilon$.  More specifically, given empirical means $\hat{\mu}_1, \ldots, \hat{\mu}_k$,
\begin{align*}
p_i(t+1) = 
\begin{cases}
1 - \epsilon + \frac{\epsilon}{k}, & \textrm{if } i = \arg \max_{j = 1, \ldots, k} \hat{\mu}_j(t) \\
\frac{\epsilon}{k}, & \textrm{otherwise}
\end{cases}
\end{align*}

The value of $\epsilon$ is a parameter in this method, and smaller values of $\epsilon$ will cause less exploration and more exploitation, whereas
larger values of $\epsilon$ will cause more exploration and less exploitation.

\subsection{Boltzmann Exploration}
In the $\epsilon$-greedy method, we do exploration by uniformly picking an arm at random; However, it would make sense to have the probability 
of picking an arm be proportional in some manner to the reward we have received from that arm this far.  This would ensure that the exploration we do 
is less random and more sensible with regard to the arms' performance so far.  This type of proportional weighting is exactly what Boltzmann Exploration 
uses.  Specifically, the probability of picking an arm is proportional to the average reward it has provided so far and these probabilities are 
set according to the Boltzmann distribution.  Again, given empirical means $\hat{\mu}_1, \ldots, \hat{\mu}_k$,
\begin{align*}
p_i(t+1) = \frac{e^{\hat{\mu}_i(t)/\tau}}{\sum_{j=1}^{k} e^{\hat{\mu}_j(t)/\tau}} \, \, \textrm{for } i = 1, \ldots, k
\end{align*}

The value of $\tau$ (oftentimes called the ``temperature'' parameter) controls the randomness of the choices.  When $\tau \to 0$, the algorithm
acts like pure greedy and continuously picks the arm with the highest observed empirical average.  When $\tau \to \infty$, the algorithm picks
arms uniformly at random.

\subsection{Poker}
The ``Price of Knowledge and Estimated Reward'' (Poker) strategy is a newer algorithm introduced in \cite{Mohri}.  Unlike the previous strategies,
Poker is a pricing strategy that assigns prices to each arm and at each round picks the arm that has the highest price.  The price for each arm 
takes into account two pieces of information: exploitation (the reward received from this arm so far) and exploration (the knowledge that could be gained
from pulling this arm).  

Let $\mu^{*} = \max(\mu_1, \ldots, \mu_k)$ be the the highest mean.  Supposing $j_{0}$ is the index of the arm with the highest observed empirical mean, 
let $\hat{\mu}^{*} = \mu_{j_{0}}$ be the actual mean of this arm.
We know $\mu^{*} \ge \hat{\mu}^{*}$ and so $\mu^{*} - \hat{\mu}^{*}$ is the reward mean improvement.  We then denote 
$\delta_{\mu} = \mathbb{E}[\mu^{*} - \hat{\mu}^{*}]$ to be the expected reward improvement.

At each round, the expected gain when pulling an arm $i$ is given by the expected reward improvement $\delta_{\mu}$ multiplied by the probability of an
improvement $\Pr[\mu_i - \hat{\mu}^{*} \ge \delta_{\mu}]$, where $\mu_i$ is the actual mean of the arm $i$.  If we let $H$ be the number of remaining rounds, 
we can exploit this knowledge gain $H$ times and so the arm pricing formula is 
$$
p_i = \hat{\mu}_i + \Pr[\mu_i - \hat{\mu}^{*} \ge \delta_{\mu}] \delta_{\mu} H,
$$
where $\hat{\mu}_i$ (the empirical average) represents the exploitation price and the second term represents the exploration price of this specific arm $i$. 

Let us now discuss the calculation of the second term.  Suppose $\hat{\mu}_{i_1} > \ldots > \hat{\mu}_{i_q}$ be the ordered observed means of levers.  
We let $\delta_u = \left(\hat{\mu}_{i_1} - \hat{\mu}_{i_{\sqrt{q}}} \right)/\sqrt{q}$.  The authors chose $\sqrt{q}$ here because they observe that it works well
in practice and because it provides some nice properties.

Let $\mathcal{N}(x, \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma}} \exp \left(\frac{(x - \mu)^2}{2 \sigma^2} \right)$ be the normal distribution.  
Let $\hat{\mu}_i$ be the observed mean estimate, $\hat{\sigma}_i$ be the standard deviation estimate, and $n_i$ be the number of times the arm has been chosen, then
we say
$$
\Pr[\mu_i \ge \hat{\mu}^{*} + \delta_u] \sim \int_{\hat{\mu}^{*} + \delta_{\mu}}^{\infty} \mathcal{N}\left(x, \hat{\mu}_{i}, \frac{\hat{\sigma}_i}{\sqrt{n_i}} \right)
$$

\section{Algorithm}

\subsection{Motivation (IN PROGRESS)}

The authors' proof that Poker is a zero regret strategy hinges upon considering an infinite horizon and thus considering the behavior of their price function as $H$ approaches infinity. However, in practice the value of $H$ will be finite either due to the fact that we know the number of rounds in advance or, as the authors suggest in the case in which the number of rounds is unknown, we consider $H$ to be a fixed parameter.

Recall that the initialization of Poker requires the algorithm to pull two random arms twice. We present a simple set of levers that will cause Poker to perform extremely poorly with a nontrivial probability.

Define the distribution $\texttt{PATHOLOGICAL}(p)$ for $p \in [0,1]$ as a set of $K$ levers with normally-distributed mean rewards in $[0,1]$ and the same standard deviation $\sigma$. We consider $(1-p)$ levers to be good (i.e. they all have rewards of 1) and the other $p$ levers to be bad. We consider the levers to have equally spaced rewards in the range $[0,\gamma]$ for some constant $\gamma$.

\noindent
\textbf{Claim} For a finite $H$, as $\sigma \rightarrow 0$, the probability that Poker running on $\texttt{PATHOLOGICAL}(p)$ will lead to a choice of arms with nearly the maximum possible regret (i.e. at least $(1-\gamma)$ each round) is $O(p^{2})$. \\ 

Suppose that Poker initially chooses to pull two distinct bad arms. We require the two bad arms to be distinct (EXPLAIN WHY). Without loss of generality, let the indices of these arms be 1 and 2. By this point, the algorithm will calculated $\hat{\mu}_{1}, \hat{\mu}_{2}, \hat{\sigma}_{1},\hat{\sigma}_{2}$ and, for all $i > 2$, will have set $\hat{\mu}_{i}= {\hat{\mu}_{1}+\hat{\mu}_{2}\over 2}$ and $\hat{\sigma}_{i} = {\hat{\sigma}_{1}+\hat{\sigma}_{2}\over 2}$

In each turn, the algorithm sets $p_{i} = \hat{\mu}_{i} + \delta(T-t)\int_{x}^{\infty}\mathcal{N}(x,\hat{\mu}_{i}, \hat{\sigma}_{i})dx$



We have assumed that $H$ is finite and, by definition $\delta$ will be finite.

SOMETHING ABOUT INTEGRALS

Thus, if sigma is arbitrarily close to $0$, $p_{i} \rightarrow \hat{\mu}_{i}$ and Poker will continue to greedily pick the lever with the highest empirical mean. Given that the unobserved levers have empirical means set equal to the average of the empirical means of levers 1 and 2, one of the two bad levers will have a strictly greater empirical mean.\footnote{We are ignoring the case that $\hat{\mu}_{1} = \hat{\mu}_{2}$ exactly because since the rewards are real numbers and their true means are different, the event of this strict equality holding is 0}

As a result, the algorithm initialized with two bad arms will greedily keep playing one of these two bad arms for the entire time game. Give that the maximum payout is 1 and the best payout of any bad arm is $\gamma$, the expected regret per round will be at least $1-\gamma$. 

Finally, the probability that Poker initializes by choosing two distinct bad arms is simply $({pK\over{K}})({p(K-1)\over{K}}) = ({K-1\over K})p^{2}$ desired.

EMPIRICAL RESULTS

\subsection{Our approach}

While the pathological distribution used in our worst-case analysis of Poker may be a somewhat artificial example, it demonstrates that in cases in which the arms' reward variances are small, the algorithm's final choices of arms could be highly dependent  on the initial choice of arms. An obvious solution to this problem is to simply initialize the algorithm with a greater number of random arms. However, we instead opted to merge the Poker algorithm with the Boltzmann algorithm.

During each round $\texttt{boltzPoker}$ (other than the four initialization rounds), the algorithm calculates the values of $p_{i}$ as in the standard Poker algorithm. Next, the algorithm maps this vector of values into a probability vector in accordance to the Boltzmann distribution and a temperature parameter in the same as the standard Boltzmann algorithm maps the mean vector into probabilities. The algorithm then chooses the arm to pull based on this distribution.\footnote{In terms of implementation, we first normalize the vector of $p_{i}$ such that all the entries lie in $[0,1]$ so as to avoid numerical overflow during the calculations.}

In terms of design, we anticipated that this modified algorithm would be more robust in dealing with the initial conditions when compared to Poker. We also believed that $p_{i}$ values, which incorporate information regarding the standard deviations and time horizon, should be a superior proxy for the "goodness" of arms in calculation of the Boltzmann probabilities when compared to just the sample means.

As the temperature parameter $\tau$ goes to 0, boltzPoker acts like the standard Poker algorithm while as $\tau \to \infty$, the algorithm converges to random guessing.

\section{Experiments}

FILLER\footnote{We have made all our code available at \texttt{https://github.com/cgeubank/COS511-BanditProject}}

\section{Conclusion}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
